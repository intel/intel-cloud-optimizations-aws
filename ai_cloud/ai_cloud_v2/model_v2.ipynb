{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import daal4py as d4p\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "from utils import Store\n",
    "from pathlib import Path\n",
    "from preprocess import get_feature_names\n",
    "from fairness import get_fairness_parity_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ea919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self, \n",
    "        backend = \"disk\", \n",
    "        bucket = None, \n",
    "        path = Path(__file__).parent.resolve()\n",
    "    ):\n",
    "        \n",
    "        self.data = pd.read_csv(\"credit_risk_dataset.csv\")\n",
    "        self.path = path / \"models\"\n",
    "        self.store = Store(\n",
    "            backend = backend, bucket = bucket, \n",
    "            path = self.path, model_name = \"model.joblib\"\n",
    "        )\n",
    "        \n",
    "    def generate_data(self, size = 4000000):\n",
    "    \n",
    "        \"\"\"\n",
    "        Function to synthetically generate 4M (default) rows \n",
    "        from loan default data.\n",
    "        \"\"\"\n",
    "\n",
    "        # synthesizing bias variable\n",
    "        bias_prob = 0.65\n",
    "        default = self.data['loan_status'] == 1\n",
    "        non_default = self.data['loan_status'] == 0\n",
    "\n",
    "        default_bias = np.random.choice(\n",
    "            [0, 1], p=[bias_prob, 1-bias_prob], size=len(default))\n",
    "        non_default_bias = np.random.choice(\n",
    "            [0, 1], p=[1-bias_prob, bias_prob], size=len(default))\n",
    "\n",
    "        # bias conditional on label\n",
    "        self.data['bias_variable'] = np.where(\n",
    "            default, default_bias, non_default_bias)\n",
    "\n",
    "        # number of rows to generate\n",
    "        if size < self.data.shape[0]:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Generating {size:,} rows of data\")\n",
    "\n",
    "            repeats = size // len(self.data)\n",
    "            self.data = self.data.loc[np.repeat(\n",
    "                self.data.index.values, repeats + 1)]\n",
    "            self.data = self.data.iloc[:size]\n",
    "\n",
    "            # perturbing all int/float columns\n",
    "            person_age = self.data['person_age'].values + \\\n",
    "                np.random.randint(-1, 1, size=len(self.data))\n",
    "            person_income = self.data['person_income'].values + \\\n",
    "                np.random.normal(0, 10, size=len(self.data))\n",
    "            person_emp_length = self.data['person_emp_length'].values + \\\n",
    "                np.random.randint(-1, 1, size=len(self.data))\n",
    "            loan_amnt = self.data['loan_amnt'].values + \\\n",
    "                np.random.normal(0, 5, size=len(self.data))\n",
    "            loan_int_rate = self.data['loan_int_rate'].values + \\\n",
    "                np.random.normal(0, 0.2, size=len(self.data))\n",
    "            loan_percent_income = self.data['loan_percent_income'].values + \\\n",
    "                (np.random.randint(0, 100, size=len(self.data)) / 1000)\n",
    "            cb_person_cred_hist_length = self.data['cb_person_cred_hist_length'].values + \\\n",
    "                np.random.randint(0, 2, size=len(self.data))\n",
    "\n",
    "            # perturbing all binary columns\n",
    "            perturb_idx = np.random.rand(len(self.data)) > 0.1\n",
    "            random_values = np.random.choice(\n",
    "                self.data['person_home_ownership'].unique(), len(self.data))\n",
    "            person_home_ownership = np.where(\n",
    "                perturb_idx, self.data['person_home_ownership'], random_values)\n",
    "\n",
    "            perturb_idx = np.random.rand(len(self.data)) > 0.1\n",
    "            random_values = np.random.choice(\n",
    "                self.data['loan_intent'].unique(), len(self.data))\n",
    "            loan_intent = np.where(\n",
    "                perturb_idx, self.data['loan_intent'], random_values)\n",
    "\n",
    "            perturb_idx = np.random.rand(len(self.data)) > 0.1\n",
    "            random_values = np.random.choice(\n",
    "                self.data['loan_grade'].unique(), len(self.data))\n",
    "            loan_grade = np.where(\n",
    "                perturb_idx, self.data['loan_grade'], random_values)\n",
    "\n",
    "            perturb_idx = np.random.rand(len(self.data)) > 0.1\n",
    "            random_values = np.random.choice(\n",
    "                self.data['cb_person_default_on_file'].unique(), len(self.data))\n",
    "            cb_person_default_on_file = np.where(\n",
    "                perturb_idx, self.data['cb_person_default_on_file'], random_values)\n",
    "\n",
    "            self.data = pd.DataFrame(list(zip(\n",
    "                person_age, person_income, person_home_ownership,\n",
    "                person_emp_length, loan_intent, loan_grade,\n",
    "                loan_amnt, loan_int_rate, self.data['loan_status'].values,\n",
    "                loan_percent_income, cb_person_default_on_file, \n",
    "                cb_person_cred_hist_length, self.data['bias_variable'].values\n",
    "            )), columns = self.data.columns)\n",
    "\n",
    "            self.data = self.data.drop_duplicates()\n",
    "            assert(len(self.data) == size)\n",
    "            self.data.reset_index(drop=True)\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to create and preprocess training and test sets\n",
    "        with 75% and 25% in each, respectively.\n",
    "\n",
    "        Input: A pandas dataframe.\n",
    "        Output: A training matrix, test set, and bias indicator\n",
    "        to evaluate model fairness.\n",
    "        \"\"\"\n",
    "\n",
    "        # create a hold-out set\n",
    "        print(\"Creating training and test sets\")\n",
    "        train, test = train_test_split(self.data,\n",
    "            test_size = 0.25, random_state = 0)\n",
    "\n",
    "        num_imputer = Pipeline(\n",
    "        steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median'))\n",
    "        ])\n",
    "        pow_transformer = PowerTransformer()\n",
    "        cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            ('num', num_imputer, ['loan_int_rate', 'person_emp_length',\n",
    "                                  'cb_person_cred_hist_length']),\n",
    "            ('pow', pow_transformer, ['person_age', 'person_income',\n",
    "                                      'loan_amnt', 'loan_percent_income']),\n",
    "            ('cat', cat_transformer, ['person_home_ownership',\n",
    "                                      'loan_intent', 'loan_grade',\n",
    "                                      'cb_person_default_on_file'])\n",
    "        ], remainder='passthrough')\n",
    "\n",
    "        # separate pipeline to allow for benchmarking\n",
    "        preprocess = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor)\n",
    "        ])\n",
    "\n",
    "        X_train = train.drop(['loan_status', 'bias_variable'], axis=1)\n",
    "        y_train = train['loan_status']\n",
    "        X_train_out = preprocess.fit_transform(X_train)\n",
    "        fnames = get_feature_names(preprocess.named_steps['preprocessor'])\n",
    "\n",
    "        # create training matrix for xgboost\n",
    "        self.dtrain = xgb.DMatrix(\n",
    "            X_train_out, y_train.values,\n",
    "            feature_names=fnames\n",
    "        )\n",
    "\n",
    "        self.bias_indicator = test['bias_variable'].values.astype(int)\n",
    "        X_test = test.drop(['loan_status', 'bias_variable'], axis=1)\n",
    "        self.y_test = test['loan_status']\n",
    "        self.X_test_out = preprocess.transform(X_test)\n",
    "        self.X_test_out = pd.DataFrame(\n",
    "            self.X_test_out, columns=fnames)\n",
    "        \n",
    "    def train(self):\n",
    "    \n",
    "        # define model\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'nthread': 4, # flags.num_cpu,\n",
    "            'tree_method': 'hist',\n",
    "            'learning_rate': 0.02,\n",
    "            'max_depth': 10,\n",
    "            'min_child_weight': 6,\n",
    "            'n_jobs': 4, # flags.num_cpu,\n",
    "            'verbosity': 0,\n",
    "            'silent': 1\n",
    "        }\n",
    "\n",
    "        print(\"Training XGBoost model\")\n",
    "        self.clf = xgb.train(params, self.dtrain, num_boost_round = 500)\n",
    "        \n",
    "    def infer(self):\n",
    "    \n",
    "        # record fairness metrics for given model on holdout test set\n",
    "        parity_values = get_fairness_parity_report(\n",
    "            self.clf, self.X_test_out, self.y_test,\n",
    "            self.bias_indicator)\n",
    "\n",
    "        model = self.store.get()\n",
    "        \n",
    "        # convert xgboost to daal model\n",
    "        print(\"Converting model to daal4py\")\n",
    "        daal_model = d4p.get_gbt_model_from_xgboost(model)\n",
    "\n",
    "        # calculate predictions\n",
    "        daal_predictions = d4p.gbt_classification_prediction(\n",
    "                nClasses=2, resultsToEvaluate=\"computeClassProbabilities\") \\\n",
    "                .compute(self.X_test_out, daal_model) \\\n",
    "                .probabilities[:, 1]\n",
    "\n",
    "        # calculate auc\n",
    "        auc = roc_auc_score(self.y_test, daal_predictions)\n",
    "        print(f\"AUC : {auc:.4f}\")\n",
    "\n",
    "        # build a text report showing the main classification metrics\n",
    "        results = classification_report(\n",
    "            self.y_test, daal_predictions > 0.5,\n",
    "            target_names = ['Non-Default', 'Default']\n",
    "        )\n",
    "        print(results)\n",
    "\n",
    "        print(\"Parity Ratios (Privileged/Non-Privileged):\")\n",
    "        for k, v in parity_values.items():\n",
    "            print(f\"\\t{k.upper()} : {v:.2f}\")\n",
    "            \n",
    "    def save(self):\n",
    "        \"\"\"pickle model.\"\"\"\n",
    "        print(\"model saved\")\n",
    "        self.store.put(self.clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfdf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Model()\n",
    "clf.generate_data(4000000)\n",
    "clf.preprocess_data()\n",
    "clf.train()\n",
    "clf.save()\n",
    "clf.infer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
